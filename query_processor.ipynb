{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load as pickle_load\n",
    "from os import path as os_path\n",
    "from string import punctuation as string_punctuation\n",
    "from re import search as regex_search\n",
    "from re import split as regex_split\n",
    "from re import sub as regex_substitute\n",
    "from heapq import heapify, heappop, heappush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this algorithm is taken from https://www.geeksforgeeks.org/python-program-for-binary-search/\n",
    "\n",
    "# Iterative Binary Search Function\n",
    "# It returns index of search_element in given dictionary arr if present,\n",
    "# else returns -1\n",
    "def binarySearch(dictionary, search_element):\n",
    "    low = 0\n",
    "    high = len(dictionary) - 1\n",
    "    mid = 0\n",
    " \n",
    "    while low <= high:\n",
    " \n",
    "        mid = (high + low) // 2\n",
    " \n",
    "        # If search_element is greater, ignore left half\n",
    "        if dictionary[mid] < search_element:\n",
    "            low = mid + 1\n",
    " \n",
    "        # If search_element is smaller, ignore right half\n",
    "        elif dictionary[mid] > search_element:\n",
    "            high = mid - 1\n",
    " \n",
    "        # means search_element is present at mid\n",
    "        else:\n",
    "            return mid\n",
    " \n",
    "    # If we reach here, then the element was not present\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClitics():\n",
    "\n",
    "    clitics = set()\n",
    "    clitic_path = os_path.join('.', 'clitics.txt')\n",
    "\n",
    "    with open(clitic_path) as stop_file:\n",
    "        stop_lines = stop_file.readlines()\n",
    "\n",
    "    for line in stop_lines:\n",
    "        stopword = line[:-1]\n",
    "        clitics.add(stopword)\n",
    "\n",
    "    return clitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary.pickle', 'rb') as dictionary_file:\n",
    "    dictionary = pickle_load(dictionary_file)\n",
    "\n",
    "dictionary_file.close()\n",
    "\n",
    "\n",
    "with open('index.pickle', 'rb') as index_file:\n",
    "    inverted_index = pickle_load(index_file)\n",
    "\n",
    "index_file.close()\n",
    "\n",
    "\n",
    "with open('stopwords.pickle', 'rb') as stopword_file:\n",
    "    stopwords = pickle_load(stopword_file)\n",
    "\n",
    "stopword_file.close()\n",
    "\n",
    "clitics = getClitics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(token_list, clitics_set):\n",
    "\n",
    "    # regular expression that will detect either a word, words with hyphens included, or a token composing of numbers\n",
    "    # with special characters('/', ':', '.', ',') like 16/20, 19.02.2021, etc.\n",
    "    shave_string = f'[{string_punctuation}]*([\\d/:.,]+\\d+)|([\\w-]+)[{string_punctuation}]*'\n",
    "\n",
    "    # a sentence will probably end with one of the following punctuation marks: '.', '!', '?', '...'\n",
    "    end_of_sentence = r'[.?!]|[...]'\n",
    "    # a flag that will show that the previous word is the last word of the sentence, which means the current word is the beginning of the sentence\n",
    "    sentence_beginning = False\n",
    "\n",
    "    # final document as a list of its tokens\n",
    "    final_document = []\n",
    "\n",
    "    tokens_after_normalization = 0\n",
    "\n",
    "    for word in token_list:\n",
    "            \n",
    "        if word.lower() not in clitics_set:\n",
    "\n",
    "            # shave the token\n",
    "            shaved = regex_search(shave_string, word)\n",
    "            if shaved is not None:\n",
    "                # as described in the definition of shave_string, token can be in one of the 2 different forms\n",
    "                token = shaved.group(1) if shaved.group(1) is not None else shaved.group(2)\n",
    "\n",
    "                # # keep terms before the normalization\n",
    "                # dictionary_before_normalization.add(token)\n",
    "\n",
    "                # detect hyphenated words\n",
    "                dash_search = regex_search('-', token)\n",
    "\n",
    "                # after this conditional block, we will have a list of tokens named as token_splitted\n",
    "                if dash_search:\n",
    "                    # if the first character of the hyphenated word is upper, like Hewett-Pickard or New York-San Fransisco, then they should be \n",
    "                    # splitted and taken as different strings. if the first character is lower, then all hyphens should be deleted and the result will be\n",
    "                    # one word('know-how' to 'knowhow')\n",
    "                    tokens_splitted = regex_split('-', token) if token[0].isupper() else [regex_substitute('-', '', token)]\n",
    "                else:\n",
    "                    # if no hyphen detected, then the token is taken directly\n",
    "                    tokens_splitted = [token]\n",
    "\n",
    "                for splitted in tokens_splitted:\n",
    "                    if splitted:\n",
    "                        # if the first character of the token is upper and it is not the beginning of the sentence, then keep the token with uppercase letters.\n",
    "                        # otherwise, lower the letters\n",
    "                        splitted = splitted if (splitted[0].isupper() and (not sentence_beginning)) else splitted.lower()\n",
    "                        \n",
    "                        # # keep terms after normalization\n",
    "                        # dictionary_after_normalization.add(splitted)\n",
    "                        # keep the number of tokens after normalization\n",
    "                        tokens_after_normalization += 1\n",
    "\n",
    "                        final_document.append(splitted)\n",
    "\n",
    "            end_of_sentence_search = regex_search(end_of_sentence, word)\n",
    "            sentence_beginning = end_of_sentence_search is not None\n",
    "        else:\n",
    "            # # keep terms before the normalization\n",
    "            # dictionary_before_normalization.add(word)\n",
    "\n",
    "            token = word.lower()\n",
    "\n",
    "            # # keep terms after normalization\n",
    "            # dictionary_after_normalization.add(word)\n",
    "            # keep the number of tokens after normalization\n",
    "            tokens_after_normalization += 1\n",
    "            \n",
    "            final_document.append(splitted)\n",
    "\n",
    "    return final_document, tokens_after_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnTopK(elements, k):\n",
    "    \n",
    "    max_heap = [(-sort_base, return_element) for return_element, sort_base in elements]\n",
    "    heapify(max_heap)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    counter = 0\n",
    "    elements_length = len(elements)\n",
    "    while counter < k and counter < elements_length:\n",
    "        sort_base, return_element = heappop(max_heap)\n",
    "        result.append((return_element, -sort_base))\n",
    "        counter += 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(first_list, second_list):\n",
    "    result = []\n",
    "\n",
    "    first_index = 0\n",
    "    second_index = 0\n",
    "    while first_index < len(first_list) and second_index < len(second_list):\n",
    "        \n",
    "        if first_list[first_index] == second_list[second_index]:\n",
    "            result.append(first_list[first_index])\n",
    "            first_index += 1\n",
    "            second_index += 1\n",
    "        \n",
    "        elif first_list[first_index] < second_list[second_index]:\n",
    "            first_index += 1\n",
    "\n",
    "        else:\n",
    "            second_index += 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeLists(document_id_lists):\n",
    "    postings_length = [(index, len(document_id_lists[index])) for index in range(len(document_id_lists))]\n",
    "    length_order = returnTopK(postings_length, len(postings_length))\n",
    "\n",
    "    result_list = document_id_lists[length_order[0][0]]\n",
    "    for i in range(1, len(length_order)):\n",
    "        index_second = length_order[i][0]\n",
    "        second_list = document_id_lists[index_second]\n",
    "\n",
    "        result_list = intersect(result_list, second_list)\n",
    "\n",
    "        if len(result_list) == 0:\n",
    "            return []\n",
    "        \n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phraseQuery(elements):\n",
    "    global clitics, inverted_index, dictionary\n",
    "\n",
    "    normalized_query, _ = normalize(elements, clitics)\n",
    "\n",
    "    document_id_lists = []\n",
    "    dictionary_positions = []\n",
    "    for term in normalized_query:\n",
    "        position = binarySearch(dictionary, term)\n",
    "\n",
    "        if position == -1:\n",
    "            return []\n",
    "        else:\n",
    "            document_ids = list(inverted_index[position][1].keys())\n",
    "            # document_ids.sort()\n",
    "            document_id_lists.append(document_ids)\n",
    "\n",
    "            dictionary_positions.append(position)\n",
    "\n",
    "    return mergeLists(document_id_lists), dictionary_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximityQuery(first_word, second_word, k):\n",
    "    global inverted_index\n",
    "    \n",
    "    lookup_lists, dictionary_indexes = phraseQuery([first_word, second_word])\n",
    "    result = []\n",
    "\n",
    "    for document_id in lookup_lists:\n",
    "        positions_first = inverted_index[dictionary_indexes[0]][1][document_id]\n",
    "        positions_second = inverted_index[dictionary_indexes[1]][1][document_id]\n",
    "\n",
    "        index_first = 0\n",
    "        index_second = 0\n",
    "        while index_first < len(positions_first) and index_second < len(positions_second):\n",
    "            \n",
    "            first_position = positions_first[index_first]\n",
    "            second_position = positions_second[index_second]\n",
    "            distance = abs(first_position - second_position)\n",
    "\n",
    "            if distance <= k:\n",
    "                result.append(document_id)\n",
    "                break\n",
    "            \n",
    "            elif first_position < second_position:\n",
    "                index_first += 1\n",
    "            else:\n",
    "                index_second += 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1299, 1623, 2436, 3981, 4898, 15043, 15686]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phraseQuery(['showers', 'continued'])\n",
    "proximityQuery('total', 'crop', 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
