{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "import html\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "source_folder = os.path.join('.', 'reuters21578')   # join two strings with path specifier independent from the OS\n",
    "\n",
    "\n",
    "# get every article as a document where the id of the document is NEWID parameter of the article\n",
    "def getDocuments():\n",
    "\n",
    "\n",
    "    for file in os.listdir(source_folder):\n",
    "        \n",
    "        # get all files with an extension .sgm\n",
    "        if file.endswith('.sgm'):\n",
    "            with open(os.path.join(source_folder, file), 'r') as f:\n",
    "                file_as_string = f.read()\n",
    "\n",
    "            # while splitting based on </REUTERS> tag, we will have (# of articles) + 1 elements in the list named reuters.\n",
    "            # the last element in the list does not contain any information because regular expression finds the closing tag\n",
    "            # for an article and splits from that point, resulting in that the last element is below the last article.\n",
    "            reuters = re.split('</REUTERS>', file_as_string)\n",
    "\n",
    "            # traverse articles one by one\n",
    "            for reuter in reuters[:-1]:\n",
    "\n",
    "                ### get the docid\n",
    "                # find the pattern NEWID=\"[number]\"> and get the number from the string.\n",
    "                # then, convert the string to an integer\n",
    "                doc_id_search = re.search(r\"NEWID=\\\"([0-9]+)\\\">\", reuter)\n",
    "                doc_id = int(doc_id_search.group(1))\n",
    "                \n",
    "\n",
    "                # get rid of html escape characters like '&lt;', '&#3;'\n",
    "                reuter = html.unescape(reuter)\n",
    "                doc = \"\"\n",
    "\n",
    "                ### get the title\n",
    "                # find the pattern \"<TEXT...<TITLE>[title_text]</TITLE>...\" and get the title_text from the string\n",
    "                title_search = re.search(r'<TEXT(.|\\n)*<TITLE>((.|\\n)*)</TITLE', reuter)\n",
    "                if title_search is not None: # if title is found\n",
    "                    title = title_search.group(2)\n",
    "                    doc += title.lower()\n",
    "\n",
    "                ### get the body\n",
    "                # find the pattern \"<TEXT...<BODY>[body_text]</BODY>...\" and get the body_text from the string\n",
    "                body_search = re.search(r'<TEXT(.|\\n)*<BODY>((.|\\n)*)</BODY>', reuter)\n",
    "                if body_search is not None: # if the body is found\n",
    "                    body = body_search.group(2)\n",
    "                    doc += \" \" + body[0].lower() + body[1:]\n",
    "\n",
    "                # if both title and body cannot be found, that means that the article is in UNPROC format and it contains\n",
    "                # only <TEXT> parameter, not <TITLE> or <BODY>\n",
    "                if (title_search is None) and (body_search is None):\n",
    "                    try:\n",
    "                        ### get the text\n",
    "                        # find the pattern \"<TEXT...[body_text]</TEXT>\" and get the body_text from the string\n",
    "                        text_search = re.search(r'<TEXT.+\\n((.|\\n)+)</TEXT>', reuter)\n",
    "                        body = text_search.group(1)\n",
    "                        doc = body[0].lower() + body[1:]\n",
    "                    except:\n",
    "                        print(reuter, '\\nerror')\n",
    "                        return\n",
    "\n",
    "                ### get rid of \"reuter\" at the end\n",
    "                # if the document contains \"Reuter\" or \"REUTER\" with some whitespace characters at the end, cut that part away from the document \n",
    "                # reuter_finish_search = re.search('((.|\\n)+)\\s*\\Z', doc)\n",
    "                # doc = reuter_finish_search.group(1) if reuter_finish_search is not None else doc\n",
    "\n",
    "                # gather the documents in an array\n",
    "                documents.append([doc_id, doc])\n",
    "\n",
    "            f.close()\n",
    "\n",
    "\n",
    "getDocuments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClitics():\n",
    "\n",
    "    clitics = set()\n",
    "    clitic_path = os.path.join('.', 'clitics.txt')\n",
    "\n",
    "    with open(clitic_path) as stop_file:\n",
    "        stop_lines = stop_file.readlines()\n",
    "\n",
    "    for line in stop_lines:\n",
    "        stopword = line[:-1]\n",
    "        clitics.add(stopword)\n",
    "\n",
    "    return clitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDocuments():\n",
    "\n",
    "    # to hold unique tokens(terms), I decided to use set data structure so that there will not be duplicate tokens at the end\n",
    "    # one for terms before the normalization(case-folding and hyphen detection) process and one for after the normalization process \n",
    "    dictionary_before_normalization = set()\n",
    "    dictionary_after_normalization = set()\n",
    "\n",
    "    # number of tokens before and after the normalization process\n",
    "    tokens_before_normalization = 0\n",
    "    tokens_after_normalization = 0\n",
    "\n",
    "\n",
    "    for document_pair_index in range(len(documents)):\n",
    "\n",
    "        # get the document as a string\n",
    "        document = documents[document_pair_index][1]\n",
    "\n",
    "        # simple splitting with respect to whitespaces\n",
    "        word_list = re.split(r'\\s+', document)\n",
    "        tokens_before_normalization += len(word_list)\n",
    "\n",
    "        # get clitics: we're, i'm, etc.\n",
    "        clitics = getClitics()\n",
    "\n",
    "        # regular expression that will detect either a word, words with hyphens included, or a token composing of numbers\n",
    "        # with special characters('/', ':', '.', ',') like 16/20, 19.02.2021, etc.\n",
    "        shave_string = f'[{string.punctuation}]*([\\d/:.,]+)|([\\w-]+)[{string.punctuation}]*'\n",
    "\n",
    "        # a sentence will probably end with one of the following punctuation marks: '.', '!', '?', '...'\n",
    "        end_of_sentence = r'[.?!]|[...]'\n",
    "        # a flag that will show that the previous word is the last word of the sentence, which means the current word is the beginning of the sentence\n",
    "        sentence_beginning = False\n",
    "\n",
    "        # final document as a list of its tokens\n",
    "        final_document = []\n",
    "\n",
    "        for word in word_list:\n",
    "            \n",
    "            if word.lower() not in clitics:\n",
    "\n",
    "                # shave the token\n",
    "                shaved = re.search(shave_string, word)\n",
    "                if shaved is not None:\n",
    "                    # as described in the definition of shave_string, token can be in one of the 2 different forms\n",
    "                    token = shaved.group(1) if shaved.group(1) is not None else shaved.group(2)\n",
    "\n",
    "                    # keep terms before the normalization\n",
    "                    dictionary_before_normalization.add(token)\n",
    "\n",
    "                    # detect hyphenated words\n",
    "                    dash_search = re.search('-', token)\n",
    "\n",
    "                    # after this conditional block, we will have a list of tokens named as token_splitted\n",
    "                    if dash_search:\n",
    "                        # if the first character of the hyphenated word is upper, like Hewett-Pickard or New York-San Fransisco, then they should be \n",
    "                        # splitted and taken as different strings. if the first character is lower, then all hyphens should be deleted and the result will be\n",
    "                        # one word('know-how' to 'knowhow')\n",
    "                        tokens_splitted = re.split('-', token) if token[0].isupper() else [re.sub('-', '', token)]\n",
    "                    else:\n",
    "                        # if no hyphen detected, then the token is taken directly\n",
    "                        tokens_splitted = [token]\n",
    "\n",
    "                    for splitted in tokens_splitted:\n",
    "                        if splitted:\n",
    "                            # if the first character of the token is upper and it is not the beginning of the sentence, then keep the token with uppercase letters.\n",
    "                            # otherwise, lower the letters\n",
    "                            splitted = splitted if (splitted[0].isupper() and (not sentence_beginning)) else splitted.lower()\n",
    "                            \n",
    "                            # keep terms after normalization\n",
    "                            dictionary_after_normalization.add(splitted)\n",
    "                            # keep the number of tokens after normalization\n",
    "                            tokens_after_normalization += 1\n",
    "\n",
    "                            final_document.append(splitted)\n",
    "\n",
    "                end_of_sentence_search = re.search(end_of_sentence, word)\n",
    "                sentence_beginning = end_of_sentence_search is not None\n",
    "            else:\n",
    "                # keep terms before the normalization\n",
    "                dictionary_before_normalization.add(word)\n",
    "\n",
    "                token = word.lower()\n",
    "\n",
    "                # keep terms after normalization\n",
    "                dictionary_after_normalization.add(word)\n",
    "                # keep the number of tokens after normalization\n",
    "                tokens_after_normalization += 1\n",
    "                \n",
    "                final_document.append(splitted)\n",
    "\n",
    "        documents[document_pair_index][1] = final_document\n",
    "\n",
    "    \n",
    "    return (dictionary_after_normalization, len(dictionary_before_normalization), tokens_before_normalization, tokens_after_normalization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dictionary, len_dictionary_before_normalization, tokens_before_normalization, tokens_after_normalization) = normalizeDocuments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86258 88660 2781148 2760861\n",
      "['bahia', 'cocoa', 'review', 'showers', 'continued', 'throughout', 'the', 'week', 'in', 'the', 'Bahia', 'cocoa', 'zone', 'alleviating', 'the', 'drought', 'since', 'early', 'January', 'and', 'improving', 'prospects', 'for', 'the', 'coming', 'temporao', 'although', 'normal', 'humidity', 'levels', 'have', 'not', 'been', 'restored', 'Comissaria', 'Smith', 'said', 'in', 'its', 'weekly', 'review', 'the', 'dry', 'period', 'means', 'the', 'temporao', 'will', 'be', 'late', 'this', 'year', 'arrivals', 'for', 'the', 'week', 'ended', 'February', '22', 'were', '155,221', 'bags', 'of', '60', 'kilos', 'making', 'a', 'cumulative', 'total', 'for', 'the', 'season', 'of', '5.93', 'mln', 'against', '5.81', 'at', 'the', 'same', 'stage', 'last', 'year', 'again', 'it', 'seems', 'that', 'cocoa', 'delivered', 'earlier', 'on', 'consignment', 'was', 'included', 'in', 'the', 'arrivals', 'figures', 'comissaria', 'Smith', 'said', 'there', 'is', 'still', 'some', 'doubt', 'as', 'to', 'how', 'much', 'old', 'crop', 'cocoa', 'is', 'still', 'available', 'as', 'harvesting', 'has', 'practically', 'come', 'to', 'an', 'end', 'with', 'total', 'Bahia', 'crop', 'estimates', 'around', '6.4', 'mln', 'bags', 'and', 'sales', 'standing', 'at', 'almost', '6.2', 'mln', 'there', 'are', 'a', 'few', 'hundred', 'thousand', 'bags', 'still', 'in', 'the', 'hands', 'of', 'farmers', 'middlemen', 'exporters', 'and', 'processors', 'there', 'are', 'doubts', 'as', 'to', 'how', 'much', 'of', 'this', 'cocoa', 'would', 'be', 'fit', 'for', 'export', 'as', 'shippers', 'are', 'now', 'experiencing', 'dificulties', 'in', 'obtaining', 'Bahia', 'superior', 'certificates', 'in', 'view', 'of', 'the', 'lower', 'quality', 'over', 'recent', 'weeks', 'farmers', 'have', 'sold', 'a', 'good', 'part', 'of', 'their', 'cocoa', 'held', 'on', 'consignment', 'comissaria', 'Smith', 'said', 'spot', 'bean', 'prices', 'rose', 'to', '340', 'to', '350', 'cruzados', 'per', 'arroba', 'of', '15', 'kilos', 'bean', 'shippers', 'were', 'reluctant', 'to', 'offer', 'nearby', 'shipment', 'and', 'only', 'limited', 'sales', 'were', 'booked', 'for', 'March', 'shipment', 'at', '1,750', 'to', '1,780', 'dlrs', 'per', 'tonne', 'to', 'ports', 'to', 'be', 'named', 'new', 'crop', 'sales', 'were', 'also', 'light', 'and', 'all', 'to', 'open', 'ports', 'with', 'June', 'going', 'at', '1,850', 'and', '1,880', 'dlrs', 'and', 'at', '35', 'and', '45', 'dlrs', 'under', 'New', 'York', 'july', 'Aug', 'at', '1,870,', '1,875', 'and', '1,880', 'dlrs', 'per', 'tonne', 'FOB', 'routine', 'sales', 'of', 'butter', 'were', 'made', 'march', 'sold', 'at', '4,340,', '4,345', 'and', '4,350', 'dlrs', 'april', 'butter', 'went', 'at', '2.27', 'times', 'New', 'York', 'May', 'June', 'at', '4,400', 'and', '4,415', 'dlrs', 'Aug', 'at', '4,351', 'to', '4,450', 'dlrs', 'and', 'at', '2.27', 'and', '2.28', 'times', 'New', 'York', 'Sept', 'and', 'Oct', 'at', '4,480', 'dlrs', 'and', '2.27', 'times', 'New', 'York', 'Dec', 'Comissaria', 'Smith', 'said', 'destinations', 'were', 'the', 'U', 'covertible', 'currency', 'areas', 'Uruguay', 'and', 'open', 'ports', 'cake', 'sales', 'were', 'registered', 'at', '785', 'to', '995', 'dlrs', 'for', 'March', '785', 'dlrs', 'for', 'May', '753', 'dlrs', 'for', 'Aug', 'and', '0.39', 'times', 'New', 'York', 'Dec', 'for', 'Oct', 'buyers', 'were', 'the', 'U', 'argentina', 'Uruguay', 'and', 'convertible', 'currency', 'areas', 'liquor', 'sales', 'were', 'limited', 'with', 'March', 'selling', 'at', '2,325', 'and', '2,380', 'dlrs', 'June', 'at', '2,375', 'dlrs', 'and', 'at', '1.25', 'times', 'New', 'York', 'July', 'Aug', 'at', '2,400', 'dlrs', 'and', 'at', '1.25', 'times', 'New', 'York', 'Sept', 'and', 'Oct', 'at', '1.25', 'times', 'New', 'York', 'Dec', 'Comissaria', 'Smith', 'said', 'total', 'Bahia', 'sales', 'are', 'currently', 'estimated', 'at', '6.13', 'mln', 'bags', 'against', 'the', '1986/87', 'crop', 'and', '1.06', 'mln', 'bags', 'against', 'the', '1987/88', 'crop', 'final', 'figures', 'for', 'the', 'period', 'to', 'February', '28', 'are', 'expected', 'to', 'be', 'published', 'by', 'the', 'Brazilian', 'Cocoa', 'Trade', 'Commission', 'after', 'carnival', 'which', 'ends', 'midday', 'on', 'February', '27.', 'reuter']\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary), len_dictionary_before_normalization, tokens_before_normalization, tokens_after_normalization)\n",
    "print(documents[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
