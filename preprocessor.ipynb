{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "import html\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "source_folder = os.path.join('.', 'reuters21578')   # join two strings with path specifier independent from the OS\n",
    "\n",
    "\n",
    "# get every article as a document where the id of the document is NEWID parameter of the article\n",
    "def getDocuments():\n",
    "\n",
    "\n",
    "    for file in os.listdir(source_folder):\n",
    "        \n",
    "        # get all files with an extension .sgm\n",
    "        if file.endswith('.sgm'):\n",
    "            with open(os.path.join(source_folder, file), 'r') as f:\n",
    "                file_as_string = f.read()\n",
    "\n",
    "            # while splitting based on </REUTERS> tag, we will have (# of articles) + 1 elements in the list named reuters.\n",
    "            # the last element in the list does not contain any information because regular expression finds the closing tag\n",
    "            # for an article and splits from that point, resulting in that the last element is below the last article.\n",
    "            reuters = re.split('</REUTERS>', file_as_string)\n",
    "\n",
    "            # traverse articles one by one\n",
    "            for reuter in reuters[:-1]:\n",
    "\n",
    "                ### get the docid\n",
    "                # find the pattern NEWID=\"[number]\"> and get the number from the string.\n",
    "                # then, convert the string to an integer\n",
    "                doc_id_search = re.search(r\"NEWID=\\\"([0-9]+)\\\">\", reuter)\n",
    "                doc_id = int(doc_id_search.group(1))\n",
    "                \n",
    "\n",
    "                # get rid of html escape characters like '&lt;', '&#3;'\n",
    "                reuter = html.unescape(reuter)\n",
    "                doc = \"\"\n",
    "\n",
    "                ### get the title\n",
    "                # find the pattern \"<TEXT...<TITLE>[title_text]</TITLE>...\" and get the title_text from the string\n",
    "                title_search = re.search(r'<TEXT(.|\\n)*<TITLE>((.|\\n)*)</TITLE', reuter)\n",
    "                if title_search is not None: # if title is found\n",
    "                    title = title_search.group(2)\n",
    "                    doc += title.lower()\n",
    "\n",
    "                ### get the body\n",
    "                # find the pattern \"<TEXT...<BODY>[body_text]</BODY>...\" and get the body_text from the string\n",
    "                body_search = re.search(r'<TEXT(.|\\n)*<BODY>((.|\\n)*)</BODY>', reuter)\n",
    "                if body_search is not None: # if the body is found\n",
    "                    body = body_search.group(2)\n",
    "                    doc += \" \" + body[0].lower() + body[1:]\n",
    "\n",
    "                # if both title and body cannot be found, that means that the article is in UNPROC format and it contains\n",
    "                # only <TEXT> parameter, not <TITLE> or <BODY>\n",
    "                if (title_search is None) and (body_search is None):\n",
    "                    try:\n",
    "                        ### get the text\n",
    "                        # find the pattern \"<TEXT...[body_text]</TEXT>\" and get the body_text from the string\n",
    "                        text_search = re.search(r'<TEXT.+\\n((.|\\n)+)</TEXT>', reuter)\n",
    "                        body = text_search.group(1)\n",
    "                        doc = body[0].lower() + body[1:]\n",
    "                    except:\n",
    "                        print(reuter, '\\nerror')\n",
    "                        return\n",
    "\n",
    "                ### get rid of \"reuter\" at the end\n",
    "                # if the document contains \"Reuter\" or \"REUTER\" with some whitespace characters at the end, cut that part away from the document \n",
    "                # reuter_finish_search = re.search('((.|\\n)+)\\s*\\Z', doc)\n",
    "                # doc = reuter_finish_search.group(1) if reuter_finish_search is not None else doc\n",
    "\n",
    "                # gather the documents in an array\n",
    "                documents.append([doc_id, doc])\n",
    "\n",
    "            f.close()\n",
    "\n",
    "\n",
    "getDocuments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClitics():\n",
    "\n",
    "    clitics = set()\n",
    "    clitic_path = os.path.join('.', 'clitics.txt')\n",
    "\n",
    "    with open(clitic_path) as stop_file:\n",
    "        stop_lines = stop_file.readlines()\n",
    "\n",
    "    for line in stop_lines:\n",
    "        stopword = line[:-1]\n",
    "        clitics.add(stopword)\n",
    "\n",
    "    return clitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bahia', 'cocoa', 'review', 'showers', 'continued', 'throughout', 'the', 'week', 'in', 'the', 'Bahia', 'cocoa', 'zone', 'alleviating', 'the', 'drought', 'since', 'early', 'January', 'and', 'improving', 'prospects', 'for', 'the', 'coming', 'temporao', 'although', 'normal', 'humidity', 'levels', 'have', 'not', 'been', 'restored', 'Comissaria', 'Smith', 'said', 'in', 'its', 'weekly', 'review', 'the', 'dry', 'period', 'means', 'the', 'temporao', 'will', 'be', 'late', 'this', 'year', 'arrivals', 'for', 'the', 'week', 'ended', 'February', '22', 'were', '155,221', 'bags', 'of', '60', 'kilos', 'making', 'a', 'cumulative', 'total', 'for', 'the', 'season', 'of', '5.93', 'mln', 'against', '5.81', 'at', 'the', 'same', 'stage', 'last', 'year', 'again', 'it', 'seems', 'that', 'cocoa', 'delivered', 'earlier', 'on', 'consignment', 'was', 'included', 'in', 'the', 'arrivals', 'figures', 'comissaria', 'Smith', 'said', 'there', 'is', 'still', 'some', 'doubt', 'as', 'to', 'how', 'much', 'old', 'crop', 'cocoa', 'is', 'still', 'available', 'as', 'harvesting', 'has', 'practically', 'come', 'to', 'an', 'end', 'with', 'total', 'Bahia', 'crop', 'estimates', 'around', '6.4', 'mln', 'bags', 'and', 'sales', 'standing', 'at', 'almost', '6.2', 'mln', 'there', 'are', 'a', 'few', 'hundred', 'thousand', 'bags', 'still', 'in', 'the', 'hands', 'of', 'farmers', 'middlemen', 'exporters', 'and', 'processors', 'there', 'are', 'doubts', 'as', 'to', 'how', 'much', 'of', 'this', 'cocoa', 'would', 'be', 'fit', 'for', 'export', 'as', 'shippers', 'are', 'now', 'experiencing', 'dificulties', 'in', 'obtaining', 'Bahia', 'superior', 'certificates', 'in', 'view', 'of', 'the', 'lower', 'quality', 'over', 'recent', 'weeks', 'farmers', 'have', 'sold', 'a', 'good', 'part', 'of', 'their', 'cocoa', 'held', 'on', 'consignment', 'comissaria', 'Smith', 'said', 'spot', 'bean', 'prices', 'rose', 'to', '340', 'to', '350', 'cruzados', 'per', 'arroba', 'of', '15', 'kilos', 'bean', 'shippers', 'were', 'reluctant', 'to', 'offer', 'nearby', 'shipment', 'and', 'only', 'limited', 'sales', 'were', 'booked', 'for', 'March', 'shipment', 'at', '1,750', 'to', '1,780', 'dlrs', 'per', 'tonne', 'to', 'ports', 'to', 'be', 'named', 'new', 'crop', 'sales', 'were', 'also', 'light', 'and', 'all', 'to', 'open', 'ports', 'with', 'June', 'going', 'at', '1,850', 'and', '1,880', 'dlrs', 'and', 'at', '35', 'and', '45', 'dlrs', 'under', 'New', 'York', 'july', 'Aug', 'at', '1,870,', '1,875', 'and', '1,880', 'dlrs', 'per', 'tonne', 'FOB', 'routine', 'sales', 'of', 'butter', 'were', 'made', 'march', 'sold', 'at', '4,340,', '4,345', 'and', '4,350', 'dlrs', 'april', 'butter', 'went', 'at', '2.27', 'times', 'New', 'York', 'May', 'June', 'at', '4,400', 'and', '4,415', 'dlrs', 'Aug', 'at', '4,351', 'to', '4,450', 'dlrs', 'and', 'at', '2.27', 'and', '2.28', 'times', 'New', 'York', 'Sept', 'and', 'Oct', 'at', '4,480', 'dlrs', 'and', '2.27', 'times', 'New', 'York', 'Dec', 'Comissaria', 'Smith', 'said', 'destinations', 'were', 'the', 'U', 'covertible', 'currency', 'areas', 'Uruguay', 'and', 'open', 'ports', 'cake', 'sales', 'were', 'registered', 'at', '785', 'to', '995', 'dlrs', 'for', 'March', '785', 'dlrs', 'for', 'May', '753', 'dlrs', 'for', 'Aug', 'and', '0.39', 'times', 'New', 'York', 'Dec', 'for', 'Oct', 'buyers', 'were', 'the', 'U', 'argentina', 'Uruguay', 'and', 'convertible', 'currency', 'areas', 'liquor', 'sales', 'were', 'limited', 'with', 'March', 'selling', 'at', '2,325', 'and', '2,380', 'dlrs', 'June', 'at', '2,375', 'dlrs', 'and', 'at', '1.25', 'times', 'New', 'York', 'July', 'Aug', 'at', '2,400', 'dlrs', 'and', 'at', '1.25', 'times', 'New', 'York', 'Sept', 'and', 'Oct', 'at', '1.25', 'times', 'New', 'York', 'Dec', 'Comissaria', 'Smith', 'said', 'total', 'Bahia', 'sales', 'are', 'currently', 'estimated', 'at', '6.13', 'mln', 'bags', 'against', 'the', '1986/87', 'crop', 'and', '1.06', 'mln', 'bags', 'against', 'the', '1987/88', 'crop', 'final', 'figures', 'for', 'the', 'period', 'to', 'February', '28', 'are', 'expected', 'to', 'be', 'published', 'by', 'the', 'Brazilian', 'Cocoa', 'Trade', 'Commission', 'after', 'carnival', 'which', 'ends', 'midday', 'on', 'February', '27.', 'reuter']\n"
     ]
    }
   ],
   "source": [
    "lst = re.split(r'\\s+', documents[0][1])\n",
    "\n",
    "# punctuation_marks = re.sub(r'[%$,/_]', '', string.punctuation)\n",
    "clitics = getClitics()\n",
    "shave_string = f'[{string.punctuation}]*([\\d/:.,]+)|([\\w-]+)[{string.punctuation}]*'\n",
    "end_of_sentence = r'[.?!]|[...]'\n",
    "sentence_beginning = False\n",
    "tokens = []\n",
    "for word in lst:\n",
    "    \n",
    "    if word.lower() not in clitics:\n",
    "\n",
    "        shaved = re.search(shave_string, word)\n",
    "        if shaved is not None:\n",
    "            token = shaved.group(1) if shaved.group(1) is not None else shaved.group(2)\n",
    "            dash_search = re.search('-', token)\n",
    "\n",
    "            if dash_search:\n",
    "                tokens_splitted = re.split('-', token) if token[0].isupper() else [re.sub('-', '', token)]\n",
    "            else:\n",
    "                tokens_splitted = [token]\n",
    "\n",
    "            for splitted in tokens_splitted:\n",
    "\n",
    "                splitted = splitted if (splitted[0].isupper() and (not sentence_beginning)) else splitted.lower()\n",
    "\n",
    "                tokens.append(splitted)\n",
    "\n",
    "        end_of_sentence_search = re.search(end_of_sentence, word)\n",
    "        sentence_beginning = end_of_sentence_search is not None\n",
    "    else:\n",
    "        token = word.lower()\n",
    "        tokens.append(token)\n",
    "\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'As']\n"
     ]
    }
   ],
   "source": [
    "end = 'As-As'\n",
    "end_of_sentence = r'-'\n",
    "search = re.split(end_of_sentence, end) if end[0].isupper() else re.sub(end_of_sentence, '', end)\n",
    "print(search)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
