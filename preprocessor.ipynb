{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "import html\n",
    "import string\n",
    "from heapq import heapify, heappop, heappush\n",
    "from math import log10\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_initial = []\n",
    "source_folder = os.path.join('.', 'reuters21578')   # join two strings with path specifier independent from the OS\n",
    "\n",
    "\n",
    "# get every article as a document where the id of the document is NEWID parameter of the article\n",
    "def getDocuments():\n",
    "\n",
    "\n",
    "    for file in os.listdir(source_folder):\n",
    "        \n",
    "        # get all files with an extension .sgm\n",
    "        if file.endswith('.sgm'):\n",
    "            with open(os.path.join(source_folder, file), 'r') as f:\n",
    "                file_as_string = f.read()\n",
    "\n",
    "            # while splitting based on </REUTERS> tag, we will have (# of articles) + 1 elements in the list named reuters.\n",
    "            # the last element in the list does not contain any information because regular expression finds the closing tag\n",
    "            # for an article and splits from that point, resulting in that the last element is below the last article.\n",
    "            reuters = re.split('</REUTERS>', file_as_string)\n",
    "\n",
    "            # traverse articles one by one\n",
    "            for reuter in reuters[:-1]:\n",
    "\n",
    "                ### get the docid\n",
    "                # find the pattern NEWID=\"[number]\"> and get the number from the string.\n",
    "                # then, convert the string to an integer\n",
    "                doc_id_search = re.search(r\"NEWID=\\\"([0-9]+)\\\">\", reuter)\n",
    "                doc_id = int(doc_id_search.group(1))\n",
    "                \n",
    "\n",
    "                # get rid of html escape characters like '&lt;', '&#3;'\n",
    "                reuter = html.unescape(reuter)\n",
    "                doc = \"\"\n",
    "\n",
    "                ### get the title\n",
    "                # find the pattern \"<TEXT...<TITLE>[title_text]</TITLE>...\" and get the title_text from the string\n",
    "                title_search = re.search(r'<TEXT(.|\\n)*<TITLE>((.|\\n)*)</TITLE', reuter)\n",
    "                if title_search is not None: # if title is found\n",
    "                    title = title_search.group(2)\n",
    "                    doc += title.lower()\n",
    "\n",
    "                ### get the body\n",
    "                # find the pattern \"<TEXT...<BODY>[body_text]</BODY>...\" and get the body_text from the string\n",
    "                body_search = re.search(r'<TEXT(.|\\n)*<BODY>((.|\\n)*)</BODY>', reuter)\n",
    "                if body_search is not None: # if the body is found\n",
    "                    body = body_search.group(2)\n",
    "                    doc += \" \" + body[0].lower() + body[1:]\n",
    "\n",
    "                # if both title and body cannot be found, that means that the article is in UNPROC format and it contains\n",
    "                # only <TEXT> parameter, not <TITLE> or <BODY>\n",
    "                if (title_search is None) and (body_search is None):\n",
    "                    try:\n",
    "                        ### get the text\n",
    "                        # find the pattern \"<TEXT...[body_text]</TEXT>\" and get the body_text from the string\n",
    "                        text_search = re.search(r'<TEXT.+\\n((.|\\n)+)</TEXT>', reuter)\n",
    "                        body = text_search.group(1)\n",
    "                        doc = body[0].lower() + body[1:]\n",
    "                    except:\n",
    "                        print(reuter, '\\nerror')\n",
    "                        return\n",
    "\n",
    "                ### get rid of \"reuter\" at the end\n",
    "                # if the document contains \"Reuter\" or \"REUTER\" with some whitespace characters at the end, cut that part away from the document \n",
    "                # reuter_finish_search = re.search('((.|\\n)+)\\s*\\Z', doc)\n",
    "                # doc = reuter_finish_search.group(1) if reuter_finish_search is not None else doc\n",
    "\n",
    "                # gather the documents in an array\n",
    "                documents_initial.append([doc_id, doc])\n",
    "\n",
    "            f.close()\n",
    "\n",
    "\n",
    "getDocuments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClitics():\n",
    "\n",
    "    clitics = set()\n",
    "    clitic_path = os.path.join('.', 'clitics.txt')\n",
    "\n",
    "    with open(clitic_path) as stop_file:\n",
    "        stop_lines = stop_file.readlines()\n",
    "\n",
    "    for line in stop_lines:\n",
    "        stopword = line[:-1]\n",
    "        clitics.add(stopword)\n",
    "\n",
    "    return clitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(token_list, clitics_set):\n",
    "\n",
    "    # regular expression that will detect either a word, words with hyphens included, or a token composing of numbers\n",
    "    # with special characters('/', ':', '.', ',') like 16/20, 19.02.2021, etc.\n",
    "    shave_string = f'[{string.punctuation}]*([\\d/:.,]+\\d+)|([\\w-]+)[{string.punctuation}]*'\n",
    "\n",
    "    # a sentence will probably end with one of the following punctuation marks: '.', '!', '?', '...'\n",
    "    end_of_sentence = r'[.?!]|[...]'\n",
    "    # a flag that will show that the previous word is the last word of the sentence, which means the current word is the beginning of the sentence\n",
    "    sentence_beginning = False\n",
    "\n",
    "    # final document as a list of its tokens\n",
    "    final_document = []\n",
    "\n",
    "    tokens_after_normalization = 0\n",
    "\n",
    "    for word in token_list:\n",
    "            \n",
    "        if word.lower() not in clitics_set:\n",
    "\n",
    "            # shave the token\n",
    "            shaved = re.search(shave_string, word)\n",
    "            if shaved is not None:\n",
    "                # as described in the definition of shave_string, token can be in one of the 2 different forms\n",
    "                token = shaved.group(1) if shaved.group(1) is not None else shaved.group(2)\n",
    "\n",
    "                # # keep terms before the normalization\n",
    "                # dictionary_before_normalization.add(token)\n",
    "\n",
    "                # detect hyphenated words\n",
    "                dash_search = re.search('-', token)\n",
    "\n",
    "                # after this conditional block, we will have a list of tokens named as token_splitted\n",
    "                if dash_search:\n",
    "                    # if the first character of the hyphenated word is upper, like Hewett-Pickard or New York-San Fransisco, then they should be \n",
    "                    # splitted and taken as different strings. if the first character is lower, then all hyphens should be deleted and the result will be\n",
    "                    # one word('know-how' to 'knowhow')\n",
    "                    tokens_splitted = re.split('-', token) if token[0].isupper() else [re.sub('-', '', token)]\n",
    "                else:\n",
    "                    # if no hyphen detected, then the token is taken directly\n",
    "                    tokens_splitted = [token]\n",
    "\n",
    "                for splitted in tokens_splitted:\n",
    "                    if splitted:\n",
    "                        # if the first character of the token is upper and it is not the beginning of the sentence, then keep the token with uppercase letters.\n",
    "                        # otherwise, lower the letters\n",
    "                        splitted = splitted if (splitted[0].isupper() and (not sentence_beginning)) else splitted.lower()\n",
    "                        \n",
    "                        # # keep terms after normalization\n",
    "                        # dictionary_after_normalization.add(splitted)\n",
    "                        # keep the number of tokens after normalization\n",
    "                        tokens_after_normalization += 1\n",
    "\n",
    "                        final_document.append(splitted)\n",
    "\n",
    "            end_of_sentence_search = re.search(end_of_sentence, word)\n",
    "            sentence_beginning = end_of_sentence_search is not None\n",
    "        else:\n",
    "            # # keep terms before the normalization\n",
    "            # dictionary_before_normalization.add(word)\n",
    "\n",
    "            token = word.lower()\n",
    "\n",
    "            # # keep terms after normalization\n",
    "            # dictionary_after_normalization.add(word)\n",
    "            # keep the number of tokens after normalization\n",
    "            tokens_after_normalization += 1\n",
    "            \n",
    "            final_document.append(splitted)\n",
    "\n",
    "    return final_document, tokens_after_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDictionary(documents):\n",
    "    dictionary = set()\n",
    "\n",
    "    for document_pair in documents:\n",
    "        document = document_pair[1]\n",
    "        \n",
    "        for token in document:\n",
    "            dictionary.add(token) if token else None\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTokens(documents, splitted):\n",
    "    \n",
    "    for document_pair in documents:\n",
    "        token_list = re.split('\\s+', document_pair[1])\n",
    "        splitted.append([document_pair[0], token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDocuments(documents):\n",
    "\n",
    "    # get clitics: we're, i'm, etc.\n",
    "    clitics = getClitics()\n",
    "    \n",
    "    tokens_before_normalization = 0\n",
    "    tokens_after_normalization = 0\n",
    "\n",
    "    normalized_documents = []\n",
    "    splitted_documents = []\n",
    "    splitTokens(documents, splitted_documents)\n",
    "\n",
    "    dictionary_before_normalization = getDictionary(splitted_documents)\n",
    "\n",
    "    for document_pair_index in range(len(splitted_documents)):\n",
    "        \n",
    "        word_list = splitted_documents[document_pair_index][1]\n",
    "        tokens_before_normalization += len(word_list)\n",
    "\n",
    "        normalized_document, normalization_tokens = normalize(word_list, clitics)\n",
    "        tokens_after_normalization += normalization_tokens\n",
    "\n",
    "        normalized_documents.append((documents[document_pair_index][0], normalized_document))\n",
    "    \n",
    "    return normalized_documents, tokens_before_normalization, tokens_after_normalization, len(dictionary_before_normalization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_documents, num_of_tokens_before_normalization, num_of_tokens_after_normalization, num_of_terms_before_normalization = normalizeDocuments(documents_initial)\n",
    "dictionary_after_normalization = getDictionary(normalized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens before normalizaion: 2781148\n",
      "Number of tokens after normalization: 2760537\n",
      "Number of terms before normalization: 130957\n",
      "Number of terms after normalization: 85209\n"
     ]
    }
   ],
   "source": [
    "print('Number of tokens before normalizaion:', num_of_tokens_before_normalization)\n",
    "print('Number of tokens after normalization:', num_of_tokens_after_normalization)\n",
    "print('Number of terms before normalization:', num_of_terms_before_normalization)\n",
    "print('Number of terms after normalization:', len(dictionary_after_normalization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dictionary = list(dictionary_after_normalization)\n",
    "final_dictionary.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this algorithm is taken from https://www.geeksforgeeks.org/python-program-for-binary-search/\n",
    "\n",
    "# Iterative Binary Search Function\n",
    "# It returns index of search_element in given dictionary arr if present,\n",
    "# else returns -1\n",
    "def binarySearch(dictionary, search_element):\n",
    "    low = 0\n",
    "    high = len(dictionary) - 1\n",
    "    mid = 0\n",
    " \n",
    "    while low <= high:\n",
    " \n",
    "        mid = (high + low) // 2\n",
    " \n",
    "        # If search_element is greater, ignore left half\n",
    "        if dictionary[mid] < search_element:\n",
    "            low = mid + 1\n",
    " \n",
    "        # If search_element is smaller, ignore right half\n",
    "        elif dictionary[mid] > search_element:\n",
    "            high = mid - 1\n",
    " \n",
    "        # means search_element is present at mid\n",
    "        else:\n",
    "            return mid\n",
    " \n",
    "    # If we reach here, then the element was not present\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInvertedIndex(term_list, normalized_documents):\n",
    "    final_inverted_index = []\n",
    "    for _ in range(len(term_list)):\n",
    "        final_inverted_index.append([0, {}])\n",
    "    \n",
    "    for document_pair in normalized_documents:\n",
    "\n",
    "        document_id = document_pair[0]\n",
    "        document = document_pair[1]\n",
    "\n",
    "        for token_index in range(len(document)):\n",
    "            token = document[token_index]\n",
    "\n",
    "            if token:\n",
    "\n",
    "                position = binarySearch(term_list, token)\n",
    "\n",
    "                if position != -1:\n",
    "                    final_inverted_index[position][0] += 1\n",
    "                    \n",
    "                    try:\n",
    "                        final_inverted_index[position][1][document_id].append(token_index)\n",
    "                    except:\n",
    "                        final_inverted_index[position][1][document_id] = [token_index]\n",
    "                    \n",
    "    return final_inverted_index\n",
    "    # print(term_list[20000], final_inverted_index[20000])\n",
    "    # print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnTopK(elements, k):\n",
    "    \n",
    "    if k >= len(elements):\n",
    "        return [return_element for return_element in elements]\n",
    "    else:\n",
    "        max_heap = [(-sort_base, return_element) for return_element, sort_base in elements]\n",
    "        heapify(max_heap)\n",
    "\n",
    "        result = []\n",
    "\n",
    "        while k > 0:\n",
    "            sort_base, return_element = heappop(max_heap)\n",
    "            result.append((return_element, -sort_base))\n",
    "            k -= 1\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequencies(inverted_index):\n",
    "\n",
    "    result = []\n",
    "    for index in range(len(inverted_index)):\n",
    "        result.append((index, inverted_index[index][0]))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_inverted_index = createInvertedIndex(final_dictionary, normalized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = getFrequencies(final_inverted_index)\n",
    "top_k_terms = returnTopK(frequencies, 100)\n",
    "\n",
    "with open('top_100_frequent_terms.txt', 'w') as f:\n",
    "    for index, frequency in top_k_terms:\n",
    "        f.write(f'{final_dictionary[index]}: {frequency} times appeared in documents\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateScores(inverted_index, num_of_documents):\n",
    "    tf_idf_scores = []\n",
    "    document_frequencies = []\n",
    "\n",
    "    for term_id in range(len(inverted_index)):\n",
    "        idf = num_of_documents / len(inverted_index[term_id][1])\n",
    "        score = inverted_index[term_id][0] * log10(idf)\n",
    "        tf_idf_scores.append((term_id, score))\n",
    "\n",
    "        document_frequencies.append(1 / idf)\n",
    "\n",
    "    return tf_idf_scores, document_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineStopWords(tf_idf_scores, threshold, document_frequencies):\n",
    "\n",
    "    total_score = 0\n",
    "    for id, score in tf_idf_scores:\n",
    "        total_score += score \n",
    "\n",
    "    stopword_scores = 0\n",
    "    stopword_list = []\n",
    "    for id, score in tf_idf_scores:\n",
    "        document_frequency = document_frequencies[id]\n",
    "\n",
    "        if document_frequency < 0.4:\n",
    "            continue\n",
    "\n",
    "        stopword_scores += score\n",
    "        stopword_list.append(id)\n",
    "       \n",
    "        if stopword_scores >= total_score * threshold:\n",
    "            break\n",
    "\n",
    "    return stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the to in a of and said for it reuter\n"
     ]
    }
   ],
   "source": [
    "tf_idf_list, document_frequencies = calculateScores(final_inverted_index, len(normalized_documents))\n",
    "top_k_tf_idf = returnTopK(tf_idf_list, 1000)\n",
    "stop_word_list = determineStopWords(top_k_tf_idf, 0.1, document_frequencies)\n",
    "\n",
    "# result = \"\"\n",
    "# for stop_word_id in stop_word_list:\n",
    "#     result += \" \" + final_dictionary[stop_word_id]\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary.pickle', 'wb') as dictionary_file:\n",
    "    pickle.dump(final_dictionary, dictionary_file)\n",
    "\n",
    "with open('index.pickle', 'wb') as index_file:\n",
    "    pickle.dump(final_inverted_index,index_file)\n",
    "\n",
    "with open('stopwords.pickle', 'wb') as stopword_file:\n",
    "    pickle.dump(stop_word_list, stopword_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
